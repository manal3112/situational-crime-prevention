# -*- coding: utf-8 -*-
"""Hypothesis_testing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IywJLxGieDLXbHAzW76oDrHequWh-cAI
"""

import pyspark
import sys
import csv
import argparse
import numpy as np
import math
from scipy.stats import t
import pprint

def create_county_tuples(tup):
  crime_data_header_dict = crime_dict.value
  fips_code = tup[crime_data_header_dict['FIPS_Code']]
  year = tup[crime_data_header_dict['Year']]
  if int(year) in range(10, 21):
    year = '20' + year
  elif int(year) in range(1, 10):
    year = '200' + year
  for key, value in crime_data_header_dict.items():
    if key not in ['FIPS_Code', 'Year']:
      yield ((int(fips_code), int(year)), (key, float(tup[value])))

def create_feature_tuples(tup):
  feature_data_header_dict = feature_dict.value
  fips_code = tup[feature_data_header_dict['FIPS_Code']]
  years = years_list.value
  for year in years:
    col_name = feature_name.value + "_" + year
    rate = tup[feature_data_header_dict[col_name]].strip()
    if len(rate) > 0:
      yield ((int(fips_code), int(year)), float(rate))

def get_correlation_val(data):
  offence_code = data[0]
  arrest_rate = []
  feature_rate = []
  for arrest_val, unemploy_val in data[1]:
    arrest_rate.append(arrest_val)
    feature_rate.append(unemploy_val)
  corr = np.corrcoef(arrest_rate, feature_rate)
  return (offence_code, (corr[0][1], len(arrest_rate) - 2))

def get_p_values(tup):
  offence_type = tup[0]
  correction_val = 43 # Number of offence codes
  pearson_corr, df = tup[1]
  t_stat = pearson_corr/math.sqrt((1 - (pearson_corr**2))/df)
  p_value = 2 * (1 - t.cdf(abs(t_stat), df))
  corrected_p_value = p_value * correction_val
  return (offence_type, (pearson_corr, p_value, corrected_p_value))

def parse_args():
  parser = argparse.ArgumentParser()
  parser.add_argument("crime_data_path", type=str)
  parser.add_argument("feature_data_path", type=str)
  parser.add_argument("feature_name", type=str)
  parser.add_argument('--years_list', nargs='+', default=['2001', '2002', '2003', '2004', '2005'])
  return parser.parse_args()

if __name__ == "__main__":
  args = parse_args()
  print(args)

  crime_data_path = args.crime_data_path
  feature_data_path = args.feature_data_path
  feature_name = args.feature_name
  years_list = args.years_list

  sc = pyspark.SparkContext()

  crime_data = sc.textFile(crime_data_path)
  feature_data = sc.textFile(feature_data_path)
  crime_data = crime_data.mapPartitions(lambda line: csv.reader(line))
  feature_data = feature_data.mapPartitions(lambda line: csv.reader(line))

  crime_data_header = crime_data.first()
  feature_data_header = feature_data.first()

  crime_data_header_dict = {k.strip(): v for v, k in enumerate(crime_data_header)}
  feature_data_header_dict = {k.strip(): v for v, k in enumerate(feature_data_header)}
  crime_data_header_dict.pop('', None)

  crime_dict = sc.broadcast(crime_data_header_dict)
  feature_dict = sc.broadcast(feature_data_header_dict)
  years_list = sc.broadcast(years_list)
  feature_name = sc.broadcast(feature_name)

  crime_temp = crime_data.filter(lambda row: row != crime_data_header).flatMap(lambda t: create_county_tuples(t))
  feature_temp = feature_data.filter(lambda row: row != feature_data_header).flatMap(lambda t: create_feature_tuples(t))

  joined_data = feature_temp.join(crime_temp)
  grouped_joined_data = joined_data.groupByKey().mapValues(list)
  flattened_grouped_joined_data = grouped_joined_data.flatMap(lambda tup: tup[1])
  flattened_grouped_joined_data = flattened_grouped_joined_data.map(lambda tup: (tup[1][0], (tup[1][1], tup[0])))
  grouped_by_offence_type = flattened_grouped_joined_data.groupByKey().mapValues(list)
  corr_list = grouped_by_offence_type.map(lambda tup: get_correlation_val(tup)).map(lambda tup: get_p_values(tup)).collect()
  corr_list = sorted(corr_list, key=lambda tup: tup[1][0], reverse=True)
  file_name = feature_name.value + "_corr_" + years_list.value[0] + "-" + years_list.value[-1] + ".txt"
  output_file = open(file_name, 'w')
  pp = pprint.PrettyPrinter(indent=2, compact=True, stream=output_file)
  pp.pprint(corr_list)
